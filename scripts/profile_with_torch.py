#!/usr/bin/env python
"""
ä½¿ç”¨ PyTorch Profiler è¿›è¡Œæ·±åº¦æ€§èƒ½åˆ†æ

æ­¤è„šæœ¬ä½¿ç”¨ PyTorch å†…ç½®çš„ Profiler å·¥å…·æ¥åˆ†æè®­ç»ƒè¿‡ç¨‹çš„è¯¦ç»†æ€§èƒ½ï¼Œ
åŒ…æ‹¬ CPU æ—¶é—´ã€CUDA æ—¶é—´ã€å†…å­˜ä½¿ç”¨ç­‰ã€‚ç»“æœå¯ä»¥åœ¨ TensorBoard ä¸­å¯è§†åŒ–ã€‚

ç”¨æ³•:
    python scripts/profile_with_torch.py --task circor_murmur
    
    # æŸ¥çœ‹ç»“æœ
    tensorboard --logdir=./profiler_logs --host 0.0.0.0 --port 6006

ä½œè€…: PA-HCL å›¢é˜Ÿ
"""

import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
from torch.profiler import profile, record_function, ProfilerActivity
import yaml


def profile_training(task: str, config_path: str, num_batches: int = 20):
    """ä½¿ç”¨ PyTorch Profiler åˆ†æè®­ç»ƒ"""
    
    print(f"\n{'='*60}")
    print(f"PyTorch Profiler æ·±åº¦æ€§èƒ½åˆ†æ")
    print(f"{'='*60}")
    print(f"ä»»åŠ¡: {task}")
    print(f"é…ç½®: {config_path}")
    print(f"åˆ†ææ‰¹æ¬¡: {num_batches}")
    print(f"{'='*60}\n")
    
    try:
        from src.trainers.downstream_trainer import DownstreamTrainer
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
        sys.exit(1)
    
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½é…ç½®å¤±è´¥: {e}")
        sys.exit(1)
    
    # è®¾ç½®ä»»åŠ¡
    if 'task' not in config:
        config['task'] = {}
    config['task']['name'] = task
    
    # è®¾ç½®å°‘é‡ epoch
    if 'downstream' not in config:
        config['downstream'] = {}
    config['downstream']['num_epochs'] = 1
    
    # åˆ›å»ºè®­ç»ƒå™¨
    try:
        trainer = DownstreamTrainer(config)
        print("âœ… è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ\n")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå™¨åˆ›å»ºå¤±è´¥: {e}")
        sys.exit(1)
    
    # è®¾ç½® profiler è¾“å‡ºç›®å½•
    profiler_dir = './profiler_logs'
    Path(profiler_dir).mkdir(exist_ok=True)
    
    print(f"å¼€å§‹æ€§èƒ½åˆ†æ (åˆ†æ {num_batches} ä¸ªæ‰¹æ¬¡)...\n")
    
    # ä½¿ç”¨ Profiler
    try:
        with profile(
            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
            record_shapes=True,
            profile_memory=True,
            with_stack=True,
            on_trace_ready=torch.profiler.tensorboard_trace_handler(profiler_dir)
        ) as prof:
            
            trainer.model.train()
            
            for batch_idx, (data, target) in enumerate(trainer.train_loader):
                if batch_idx >= num_batches:
                    break
                
                # æ•°æ®ä¼ è¾“
                with record_function("data_transfer"):
                    data = data.to(trainer.device)
                    target = target.to(trainer.device)
                
                # å‰å‘ä¼ æ’­
                with record_function("forward"):
                    if trainer.use_amp:
                        with torch.cuda.amp.autocast():
                            output = trainer.model(data)
                            loss = trainer.criterion(output, target)
                    else:
                        output = trainer.model(data)
                        loss = trainer.criterion(output, target)
                
                # åå‘ä¼ æ’­
                with record_function("backward"):
                    trainer.optimizer.zero_grad()
                    if trainer.use_amp:
                        trainer.scaler.scale(loss).backward()
                    else:
                        loss.backward()
                
                # ä¼˜åŒ–å™¨æ­¥éª¤
                with record_function("optimizer_step"):
                    if trainer.use_amp:
                        trainer.scaler.step(trainer.optimizer)
                        trainer.scaler.update()
                    else:
                        trainer.optimizer.step()
                
                # é€šçŸ¥ profiler ä¸€ä¸ªæ­¥éª¤å®Œæˆ
                prof.step()
                
                if (batch_idx + 1) % 5 == 0:
                    print(f"  å·²åˆ†æ {batch_idx + 1}/{num_batches} ä¸ªæ‰¹æ¬¡...")
        
        print("\nâœ… æ€§èƒ½åˆ†æå®Œæˆ!\n")
        
        # è¾“å‡ºç»Ÿè®¡è¡¨æ ¼
        print("="*60)
        print("CPU æ—¶é—´ç»Ÿè®¡ (Top 10)")
        print("="*60)
        print(prof.key_averages().table(
            sort_by="cpu_time_total",
            row_limit=10
        ))
        
        print("\n" + "="*60)
        print("CUDA æ—¶é—´ç»Ÿè®¡ (Top 10)")
        print("="*60)
        print(prof.key_averages().table(
            sort_by="cuda_time_total",
            row_limit=10
        ))
        
        print("\n" + "="*60)
        print("å†…å­˜ä½¿ç”¨ç»Ÿè®¡ (Top 10)")
        print("="*60)
        print(prof.key_averages().table(
            sort_by="self_cpu_memory_usage",
            row_limit=10
        ))
        
        # ä¿å­˜è¯¦ç»†ç»Ÿè®¡åˆ°æ–‡ä»¶
        stats_file = Path(profiler_dir) / "profiler_stats.txt"
        with open(stats_file, 'w') as f:
            f.write("="*60 + "\n")
            f.write("CPU æ—¶é—´ç»Ÿè®¡ (Top 20)\n")
            f.write("="*60 + "\n")
            f.write(prof.key_averages().table(
                sort_by="cpu_time_total",
                row_limit=20
            ))
            f.write("\n\n")
            
            f.write("="*60 + "\n")
            f.write("CUDA æ—¶é—´ç»Ÿè®¡ (Top 20)\n")
            f.write("="*60 + "\n")
            f.write(prof.key_averages().table(
                sort_by="cuda_time_total",
                row_limit=20
            ))
            f.write("\n\n")
            
            f.write("="*60 + "\n")
            f.write("å†…å­˜ä½¿ç”¨ç»Ÿè®¡ (Top 20)\n")
            f.write("="*60 + "\n")
            f.write(prof.key_averages().table(
                sort_by="self_cpu_memory_usage",
                row_limit=20
            ))
        
        print(f"\nğŸ“„ è¯¦ç»†ç»Ÿè®¡å·²ä¿å­˜åˆ°: {stats_file.absolute()}")
        print(f"\nğŸ“Š TensorBoard æ—¥å¿—ç›®å½•: {Path(profiler_dir).absolute()}")
        print("\n" + "="*60)
        print("æŸ¥çœ‹ TensorBoard å¯è§†åŒ–:")
        print("="*60)
        print(f"  tensorboard --logdir={profiler_dir} --host 0.0.0.0 --port 6006")
        print("  ç„¶ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€: http://localhost:6006")
        print("="*60 + "\n")
        
    except Exception as e:
        print(f"\nâŒ æ€§èƒ½åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description='ä½¿ç”¨ PyTorch Profiler è¿›è¡Œæ·±åº¦æ€§èƒ½åˆ†æ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•
  python scripts/profile_with_torch.py --task circor_murmur
  
  # åˆ†ææ›´å¤šæ‰¹æ¬¡
  python scripts/profile_with_torch.py --task circor_murmur --batches 50
  
  # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
  python scripts/profile_with_torch.py --task physionet2016 \\
      --config configs/finetune.yaml
  
  # æŸ¥çœ‹ç»“æœ
  tensorboard --logdir=./profiler_logs --host 0.0.0.0 --port 6006
        """
    )
    
    parser.add_argument(
        '--task',
        type=str,
        required=True,
        help='ä»»åŠ¡åç§° (circor_murmur, circor_outcome, physionet2016, pascal)'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        default='configs/finetune.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (default: configs/finetune.yaml)'
    )
    
    parser.add_argument(
        '--batches',
        type=int,
        default=20,
        help='åˆ†æçš„æ‰¹æ¬¡æ•°é‡ (default: 20)'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_path = Path(args.config)
    if not config_path.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        sys.exit(1)
    
    # æ‰§è¡Œåˆ†æ
    profile_training(args.task, str(config_path), args.batches)


if __name__ == '__main__':
    main()
