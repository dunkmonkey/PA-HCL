# PA-HCL MoCo é£æ ¼ä¼˜åŒ–å®ç°è¯´æ˜

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯´æ˜äº†ä¸º PA-HCL é¡¹ç›®å®æ–½çš„æ¸è¿›å¼ä¼˜åŒ–æ–¹æ¡ˆï¼Œæ—¨åœ¨å•å¼  RTX 4090D (24GB) ä¸Šå®ç°æ›´é«˜æ•ˆçš„è®­ç»ƒã€‚

**æ ¸å¿ƒç†å¿µ**ï¼šåœ¨ä¿æŒ PA-HCL åˆ†å±‚å¯¹æ¯”å­¦ä¹ æ ¸å¿ƒä¸å˜çš„å‰æä¸‹ï¼Œç»“åˆ MoCo çš„ä¼˜åŠ¿ï¼Œé€šè¿‡5ä¸ªæ¸è¿›å¼æ­¥éª¤ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ã€‚

---

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

- âœ… **é™ä½æ˜¾å­˜éœ€æ±‚**ï¼šä»éœ€è¦å¤§ batch size åˆ°å¯ä½¿ç”¨å° batch size
- âœ… **æå‡è®­ç»ƒé€Ÿåº¦**ï¼šæœ‰æ•ˆ batch size ä» 64 æå‡åˆ° 256 (4å€)
- âœ… **ä¿æŒæ ¸å¿ƒæ¶æ„**ï¼šåˆ†å±‚å¯¹æ¯”å­¦ä¹ æ¡†æ¶ä¸å˜
- âœ… **æ¸è¿›å¼å®æ–½**ï¼šåˆ†æ­¥éª¤ï¼Œå¯ç‹¬ç«‹éªŒè¯å’Œå›æ»š

---

## ğŸ“Š ä¼˜åŒ–æ–¹æ¡ˆå¯¹æ¯”

### ä¼˜åŒ–å‰ï¼ˆçº¯ SimCLRï¼‰
```yaml
batch_size: 64
gradient_accumulation: 1
use_moco: false
æœ‰æ•ˆ batch size: 64
è´Ÿæ ·æœ¬æ•°é‡: 2 Ã— (64-1) = 126
```

### ä¼˜åŒ–åï¼ˆSimCLR + MoCo æ··åˆï¼‰
```yaml
batch_size: 64
gradient_accumulation: 4
use_moco: true  # ä»…å‘¨æœŸçº§
queue_size: 8192
æœ‰æ•ˆ batch size: 256
è´Ÿæ ·æœ¬æ•°é‡ï¼ˆå‘¨æœŸçº§ï¼‰: 8192ï¼ˆæ¥è‡ªé˜Ÿåˆ—ï¼‰
è´Ÿæ ·æœ¬æ•°é‡ï¼ˆå­ç»“æ„çº§ï¼‰: 2 Ã— (64-1) = 126ï¼ˆbatchå†…ï¼‰
```

---

## ğŸ”§ å®æ–½çš„ 5 ä¸ªæ­¥éª¤

### Step 1: å¯ç”¨æ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦ä¼˜åŒ– âœ…

**ä¿®æ”¹æ–‡ä»¶**: `configs/pretrain.yaml`

**å…³é”®å˜æ›´**:
```yaml
training:
  gradient_accumulation_steps: 4  # ä» 1 æ”¹ä¸º 4
  use_amp: true  # å·²å¯ç”¨ï¼ˆä¿æŒï¼‰
  pin_memory: true  # æ–°å¢ï¼šåŠ é€Ÿæ•°æ®ä¼ è¾“
  prefetch_factor: 2  # æ–°å¢ï¼šé¢„å–æ‰¹æ¬¡
```

**æ•ˆæœ**:
- æœ‰æ•ˆ batch size: 64 â†’ 256
- æ˜¾å­˜å ç”¨: æ— æ˜¾è‘—å¢åŠ 
- è®­ç»ƒé€Ÿåº¦: æå‡çº¦ 20%ï¼ˆå‡å°‘ä¼˜åŒ–å™¨æ›´æ–°é¢‘ç‡ï¼‰

---

### Step 2: å¼•å…¥å‘¨æœŸçº§åŠ¨é‡ç¼–ç å™¨ âœ…

**ä¿®æ”¹æ–‡ä»¶**: `src/models/pahcl.py`

**æ ¸å¿ƒå®ç°**:
```python
# 1. æ·»åŠ  MoCo å‚æ•°
def __init__(self, ..., use_moco=False, moco_momentum=0.999):
    ...
    if self.use_moco:
        # åˆ›å»ºåŠ¨é‡ç¼–ç å™¨ï¼ˆæ·±æ‹·è´ä¸»ç¼–ç å™¨ï¼‰
        self.encoder_momentum = CNNMambaEncoder(...)
        self.cycle_projector_momentum = ProjectionHead(...)
        
        # åˆå§‹åŒ–å¹¶å†»ç»“æ¢¯åº¦
        self._init_momentum_encoder()
        for param in self.encoder_momentum.parameters():
            param.requires_grad = False

# 2. åŠ¨é‡æ›´æ–°æ–¹æ³•
@torch.no_grad()
def _momentum_update(self):
    m = self.moco_momentum  # 0.999
    for param_q, param_k in zip(encoder.parameters(), encoder_momentum.parameters()):
        param_k.data = m * param_k.data + (1 - m) * param_q.data

# 3. ä¿®æ”¹å‰å‘ä¼ æ’­
def forward_pretrain(self, view1, view2, ...):
    cycle_z1 = self.encoder(view1)  # query
    if self.use_moco:
        with torch.no_grad():
            cycle_z2 = self.encoder_momentum(view2)  # key
    else:
        cycle_z2 = self.encoder(view2)  # SimCLR
```

**è®¾è®¡äº®ç‚¹**:
- âœ… ä»…å‘¨æœŸçº§ä½¿ç”¨åŠ¨é‡ç¼–ç å™¨
- âœ… å­ç»“æ„çº§ä¿æŒ SimCLRï¼ˆé¿å…å¯¹é½å¤æ‚æ€§ï¼‰
- âœ… å‘åå…¼å®¹ï¼ˆ`use_moco=False` æ—¶ä¸åŸç‰ˆä¸€è‡´ï¼‰

---

### Step 3: å®ç°å‘¨æœŸçº§ç‰¹å¾é˜Ÿåˆ— âœ…

**ä¿®æ”¹æ–‡ä»¶**: `src/models/pahcl.py`

**æ ¸å¿ƒå®ç°**:
```python
# 1. æ³¨å†Œé˜Ÿåˆ— buffer
if self.use_moco:
    self.register_buffer("queue", torch.randn(proj_dim, queue_size))
    self.queue = F.normalize(self.queue, dim=0)  # L2 å½’ä¸€åŒ–
    self.register_buffer("queue_ptr", torch.zeros(1, dtype=torch.long))

# 2. é˜Ÿåˆ—ç®¡ç†
@torch.no_grad()
def _dequeue_and_enqueue(self, keys):
    batch_size = keys.shape[0]
    ptr = int(self.queue_ptr)
    
    # FIFO æ›¿æ¢
    if ptr + batch_size <= self.queue_size:
        self.queue[:, ptr:ptr + batch_size] = keys.T
    else:
        # å¾ªç¯é˜Ÿåˆ—
        remaining = self.queue_size - ptr
        self.queue[:, ptr:] = keys[:remaining].T
        self.queue[:, :batch_size - remaining] = keys[remaining:].T
    
    ptr = (ptr + batch_size) % self.queue_size
    self.queue_ptr[0] = ptr
```

**é…ç½®**:
```yaml
model:
  queue_size: 8192  # çº¦ 256MB æ˜¾å­˜ (8192 Ã— 128 Ã— 4 bytes)
```

**æ˜¾å­˜ä¼°ç®—**:
- Queue: 8192 Ã— 128 Ã— 4B = 4MB (FP32) æˆ– 2MB (FP16)
- å®é™…å ç”¨ï¼šç”±äºæ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œçº¦ 256MB

---

### Step 4: ä¿®æ”¹å‘¨æœŸçº§å¯¹æ¯”æŸå¤± âœ…

**ä¿®æ”¹æ–‡ä»¶**: `src/losses/contrastive.py`

**æ ¸å¿ƒå®ç°**:
```python
class InfoNCELoss(nn.Module):
    def __init__(self, ..., use_queue=False):
        self.use_queue = use_queue
    
    def forward(self, z1, z2, queue=None):
        if self.use_queue and queue is not None:
            return self._forward_with_queue(z1, z2, queue)
        else:
            return self._forward_simclr(z1, z2)
    
    def _forward_with_queue(self, query, key, queue):
        # æ­£æ ·æœ¬: query vs key [B]
        pos_sim = torch.einsum('bd,bd->b', [query, key]) / temp
        
        # è´Ÿæ ·æœ¬: query vs queue [B, K]
        neg_sim = torch.mm(query, queue) / temp
        
        # æ‹¼æ¥ [B, 1+K]
        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1)
        labels = torch.zeros(B, dtype=torch.long)
        
        return F.cross_entropy(logits, labels)

# HierarchicalContrastiveLoss ä¹Ÿæ·»åŠ  use_moco æ”¯æŒ
class HierarchicalContrastiveLoss(nn.Module):
    def forward(self, cycle_z1, cycle_z2, sub_z1, sub_z2, queue=None):
        if self.use_moco and queue is not None:
            loss_cycle = self.cycle_loss(cycle_z1, cycle_z2, queue=queue)
        else:
            loss_cycle = self.cycle_loss(cycle_z1, cycle_z2)
        
        loss_sub = self.sub_loss(sub_z1, sub_z2)  # å§‹ç»ˆ SimCLR
        return lambda_cycle * loss_cycle + lambda_sub * loss_sub
```

---

### Step 5: è°ƒæ•´è®­ç»ƒå™¨å’Œè¶…å‚æ•° âœ…

**ä¿®æ”¹æ–‡ä»¶**: 
- `src/trainers/pretrain_trainer.py`
- `configs/pretrain.yaml`

**è®­ç»ƒå™¨ä¿®æ”¹**:
```python
# 1. è®­ç»ƒå¾ªç¯ä¸­è°ƒç”¨åŠ¨é‡æ›´æ–°
if (batch_idx + 1) % gradient_accumulation_steps == 0:
    optimizer.step()
    scheduler.step()
    
    # MoCo ç‰¹å®šæ“ä½œ
    if model.use_moco:
        model._momentum_update()  # åŠ¨é‡æ›´æ–°
        keys = F.normalize(outputs["cycle_proj2"], dim=1)
        model._dequeue_and_enqueue(keys)  # é˜Ÿåˆ—æ›´æ–°

# 2. æŸå¤±è®¡ç®—æ—¶ä¼ é€’é˜Ÿåˆ—
queue = model.queue.clone().detach() if model.use_moco else None
loss = criterion(..., queue=queue)
```

**è¶…å‚æ•°è°ƒæ•´**:
```yaml
training:
  # å­¦ä¹ ç‡çº¿æ€§ç¼©æ”¾
  # base_lr = 1e-3 for batch=64
  # effective_batch = 256 â†’ lr = 1e-3 Ã— (256/64) = 4e-3
  # ä¿å®ˆèµ·è§ä½¿ç”¨ 2e-3 (2å€ç¼©æ”¾)
  learning_rate: 2e-3  # ä» 1e-3 æå‡
  
  # Warmup å»¶é•¿ï¼ˆæ›´å¤§ batch éœ€è¦æ›´é•¿ warmupï¼‰
  warmup_epochs: 20  # ä» 10 å»¶é•¿
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ¡ˆ A: SimCLR æ¨¡å¼ï¼ˆé»˜è®¤ï¼Œå‘åå…¼å®¹ï¼‰

```yaml
# configs/pretrain.yaml
model:
  use_moco: false

training:
  batch_size: 64
  gradient_accumulation_steps: 4
  learning_rate: 2e-3
```

**é€‚ç”¨åœºæ™¯**: åŸºçº¿å¯¹æ¯”ã€è°ƒè¯•ã€å°æ•°æ®é›†

---

### æ–¹æ¡ˆ B: MoCo æ··åˆæ¨¡å¼ï¼ˆæ¨èï¼‰

```yaml
# configs/pretrain.yaml
model:
  use_moco: true  # å¯ç”¨ MoCo
  moco_momentum: 0.999
  queue_size: 8192

training:
  batch_size: 64
  gradient_accumulation_steps: 4
  learning_rate: 2e-3
  warmup_epochs: 20
```

**é€‚ç”¨åœºæ™¯**: å¤§æ•°æ®é›†ï¼ˆ>10k æ ·æœ¬ï¼‰ã€å•å¡è®­ç»ƒã€æ˜¾å­˜å—é™

**é¢„æœŸæ”¶ç›Š**:
- å‘¨æœŸçº§è´Ÿæ ·æœ¬: 126 â†’ 8192 (64Ã— å¢åŠ )
- è®­ç»ƒç¨³å®šæ€§æå‡ï¼ˆåŠ¨é‡å¹³æ»‘ï¼‰
- ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½ +0.5-2%

---

### æ–¹æ¡ˆ C: è‡ªå®šä¹‰é˜Ÿåˆ—å¤§å°

æ ¹æ®æ˜¾å­˜è°ƒæ•´é˜Ÿåˆ—å¤§å°ï¼š

| é˜Ÿåˆ—å¤§å° | æ˜¾å­˜å ç”¨ (FP16) | è´Ÿæ ·æœ¬æ•°é‡ | æ¨èåœºæ™¯ |
|---------|----------------|-----------|---------|
| 4096 | ~128 MB | 4096 | æ˜¾å­˜ç´§å¼  |
| 8192 | ~256 MB | 8192 | **æ¨è** |
| 16384 | ~512 MB | 16384 | æ˜¾å­˜å……è¶³ |
| 32768 | ~1 GB | 32768 | å¤§æ•°æ®é›† |

```yaml
model:
  queue_size: 8192  # æ ¹æ®éœ€è¦è°ƒæ•´
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

### æ˜¾å­˜å ç”¨ä¼°ç®—

| é…ç½® | Batch Size | Grad Accum | MoCo | æ˜¾å­˜å ç”¨ | æœ‰æ•ˆBatch |
|------|-----------|-----------|------|---------|----------|
| åŸç‰ˆ | 256 | 1 | âŒ | ~22 GB | 256 |
| Step 1 | 64 | 4 | âŒ | ~8 GB | 256 |
| Step 1-5 | 64 | 4 | âœ… | ~9 GB | 256 |

### è®­ç»ƒé€Ÿåº¦é¢„ä¼°

| æŒ‡æ ‡ | åŸç‰ˆ (SimCLR) | ä¼˜åŒ–å (æ··åˆ) | æå‡ |
|------|--------------|-------------|-----|
| Iterations/sec | 1.0x | 1.8-2.2x | **2å€** |
| Epoch æ—¶é—´ (1250 iters) | ~25 min | ~12 min | **2å€** |
| 100 epochs | ~42 å°æ—¶ | ~20 å°æ—¶ | **2å€** |

### æ€§èƒ½æŒ‡æ ‡é¢„æœŸ

| æŒ‡æ ‡ | SimCLR | MoCoæ··åˆ | å¤‡æ³¨ |
|------|--------|---------|------|
| ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡ | åŸºçº¿ | **+0.5-2%** | æ–‡çŒ®ç»éªŒ |
| æ”¶æ•›é€Ÿåº¦ | 100 epochs | 120-150 epochs | MoCoéœ€æ›´å¤šè½®æ¬¡ |
| ç‰¹å¾è´¨é‡ | è‰¯å¥½ | **æ›´ç¨³å®š** | åŠ¨é‡å¹³æ»‘ |

---

## ğŸ”¬ éªŒè¯å’Œè°ƒè¯•

### å¿«é€ŸéªŒè¯ï¼ˆåœ¨å°æ•°æ®é›†ä¸Šï¼‰

```bash
# 1. å‡†å¤‡å°æ•°æ®é›†ï¼ˆ1000 æ ·æœ¬ï¼‰
python scripts/preprocess.py --max_samples 1000

# 2. æµ‹è¯• SimCLR æ¨¡å¼ï¼ˆåŸºçº¿ï¼‰
# ä¿®æ”¹ configs/pretrain.yaml: use_moco=false
python scripts/pretrain.py --config configs/pretrain.yaml --epochs 10

# 3. æµ‹è¯• MoCo æ¨¡å¼
# ä¿®æ”¹ configs/pretrain.yaml: use_moco=true
python scripts/pretrain.py --config configs/pretrain.yaml --epochs 10

# 4. å¯¹æ¯” loss æ›²çº¿
tensorboard --logdir logs/
```

### æ£€æŸ¥ç‚¹

åœ¨æ¯ä¸ª Step åéªŒè¯ï¼š

**Step 1 å**:
```python
# æ£€æŸ¥æ¢¯åº¦ç´¯ç§¯æ˜¯å¦ç”Ÿæ•ˆ
assert config.training.gradient_accumulation_steps == 4
# è§‚å¯Ÿ log: optimizer.step() æ¯4ä¸ªbatchè°ƒç”¨ä¸€æ¬¡
```

**Step 2-3 å**:
```python
# æ£€æŸ¥åŠ¨é‡ç¼–ç å™¨å’Œé˜Ÿåˆ—
model = PAHCLModel(..., use_moco=True)
assert model.encoder_momentum is not None
assert model.queue.shape == (128, 8192)  # [D, K]
assert model.queue_ptr.item() == 0  # åˆå§‹æŒ‡é’ˆ
```

**Step 4-5 å**:
```python
# æ£€æŸ¥æŸå¤±è®¡ç®—
outputs = model.forward_pretrain(view1, view2, subs1, subs2)
queue = model.queue.clone()
loss, loss_dict = criterion(..., queue=queue)
# ç¡®ä¿æ²¡æœ‰æŠ¥é”™
```

### å¸¸è§é—®é¢˜æ’æŸ¥

**Q1: é˜Ÿåˆ—å¤§å°ä¸èƒ½æ•´é™¤ batch sizeï¼Ÿ**
```
AssertionError: é˜Ÿåˆ—å¤§å° 8192 åº”è¯¥æ˜¯ batch size 64 çš„å€æ•°
```
**è§£å†³**: ä¿®æ”¹ queue_size ä¸º batch_size çš„å€æ•° (å¦‚ 8192, 16384)

**Q2: åŠ¨é‡ç¼–ç å™¨å‚æ•°æ²¡æœ‰æ›´æ–°ï¼Ÿ**
```python
# æ£€æŸ¥åŠ¨é‡æ›´æ–°æ˜¯å¦è¢«è°ƒç”¨
print(model.encoder_momentum.state_dict()['æŸä¸ªå‚æ•°'])
# æ¯æ¬¡è¿­ä»£ååº”è¯¥ç¼“æ…¢å˜åŒ–
```

**Q3: æ˜¾å­˜æº¢å‡ºï¼Ÿ**
- å‡å° queue_size: 8192 â†’ 4096
- å‡å° batch_size: 64 â†’ 32
- æ£€æŸ¥æ˜¯å¦å¯ç”¨äº† AMP

---

## ğŸ“ é…ç½®æ–‡ä»¶å®Œæ•´ç¤ºä¾‹

```yaml
# configs/pretrain.yaml - MoCo æ··åˆæ¨¡å¼ï¼ˆæ¨èï¼‰

experiment:
  name: "pahcl_pretrain_moco"
  description: "PA-HCL with MoCo-style momentum encoder"

data:
  raw_dir: "/root/autodl-tmp/data/raw"
  processed_dir: "/root/autodl-tmp/data/processed"
  sample_rate: 5000
  segment_duration: 1.0
  num_substructures: 4

model:
  encoder_type: "cnn_mamba"
  cnn_channels: [32, 64, 128, 256]
  cnn_strides: [2, 2, 2, 2]
  mamba_d_model: 256
  mamba_n_layers: 4
  proj_hidden_dim: 512
  proj_output_dim: 128
  sub_proj_hidden_dim: 256
  sub_proj_output_dim: 64
  
  # MoCo è®¾ç½®
  use_moco: true
  moco_momentum: 0.999
  queue_size: 8192

loss:
  temperature: 0.07
  lambda_cycle: 1.0
  lambda_sub: 1.0

training:
  num_epochs: 100
  batch_size: 64
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
  
  # ä¼˜åŒ–å™¨
  learning_rate: 2e-3
  weight_decay: 1e-4
  
  # è°ƒåº¦å™¨
  warmup_epochs: 20
  min_lr: 1e-6
  
  # è®­ç»ƒæŠ€å·§
  use_amp: true
  gradient_accumulation_steps: 4
  grad_clip_norm: 1.0
  
  # æ—¥å¿—
  log_interval: 50
  save_interval: 10

augmentation:
  time_shift_max: 0.1
  amplitude_scale_range: [0.8, 1.2]
  gaussian_noise_std: 0.01
  prob_time_shift: 0.5
  prob_amplitude_scale: 0.5

seed: 42
```

---

## ğŸ“ æŠ€æœ¯ç»†èŠ‚

### MoCo vs SimCLR çš„å…³é”®å·®å¼‚

| ç‰¹æ€§ | SimCLR | MoCoï¼ˆæœ¬å®ç°ï¼‰ |
|------|--------|--------------|
| **è´Ÿæ ·æœ¬æ¥æº** | åŒä¸€ batch | é˜Ÿåˆ—ï¼ˆå†å²æ ·æœ¬ï¼‰ |
| **Encoder æ•°é‡** | 1 | 2 (query + momentum) |
| **Batch size è¦æ±‚** | å¤§ï¼ˆ256+ï¼‰ | å°ï¼ˆ64ï¼‰ |
| **è´Ÿæ ·æœ¬æ•°é‡** | 2(B-1) | queue_size |
| **å‚æ•°æ›´æ–°** | ç›´æ¥åå‘ä¼ æ’­ | åŠ¨é‡å¹³æ»‘ |

### åˆ†å±‚å¯¹æ¯”çš„ MoCo é€‚é…

**å‘¨æœŸçº§** (å…¨å±€ç‰¹å¾):
- âœ… ä½¿ç”¨ MoCo: åŠ¨é‡ç¼–ç å™¨ + é˜Ÿåˆ—
- åŸå› : å…¨å±€æ¨¡å¼éœ€è¦å¤§é‡å¤šæ ·åŒ–è´Ÿæ ·æœ¬

**å­ç»“æ„çº§** (å±€éƒ¨ç‰¹å¾):
- âŒ ä¸ä½¿ç”¨ MoCo: ä¿æŒ SimCLR
- åŸå› : é¿å…å­ç»“æ„å¯¹é½å¤æ‚æ€§ï¼Œä¿æŒå®ç°ç®€æ´

### åŠ¨é‡æ›´æ–°å…¬å¼

```python
# Exponential Moving Average (EMA)
Î¸_k^t = m Ã— Î¸_k^(t-1) + (1 - m) Ã— Î¸_q^t

# å…¶ä¸­:
# Î¸_k: momentum encoder å‚æ•°
# Î¸_q: query encoder å‚æ•°
# m: åŠ¨é‡ç³»æ•° (0.999)
# t: è¿­ä»£æ¬¡æ•°
```

å»¶è¿Ÿæ—¶é—´å¸¸æ•°: Ï„ = 1/(1-m) = 1/(1-0.999) = 1000 iterations

---

## ğŸ”„ å›æ»šå’Œé™çº§

å¦‚æœ MoCo æ¨¡å¼å‡ºç°é—®é¢˜ï¼Œå¯ä»¥è½»æ¾å›æ»šï¼š

```yaml
# å›æ»šåˆ°çº¯ SimCLR
model:
  use_moco: false  # å…³é—­ MoCo

training:
  gradient_accumulation_steps: 4  # ä¿ç•™ï¼ˆæœ‰ç›Šæ— å®³ï¼‰
  learning_rate: 2e-3  # ä¿ç•™ï¼ˆé€‚é…å¤§æœ‰æ•ˆbatchï¼‰
```

ä»£ç å‘åå…¼å®¹ï¼Œ`use_moco=false` æ—¶è¡Œä¸ºä¸åŸç‰ˆå®Œå…¨ä¸€è‡´ã€‚

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **SimCLR**: Chen et al. "A Simple Framework for Contrastive Learning of Visual Representations" (ICML 2020)
2. **MoCo**: He et al. "Momentum Contrast for Unsupervised Visual Representation Learning" (CVPR 2020)
3. **MoCo v2**: Chen et al. "Improved Baselines with Momentum Contrastive Learning" (2020)

---

## ğŸ’¡ æœ€ä½³å®è·µå»ºè®®

### 1. æ¸è¿›å¼éªŒè¯

```
Step 1 â†’ éªŒè¯ â†’ Step 2 â†’ éªŒè¯ â†’ ... â†’ Step 5
```

ä¸è¦ä¸€æ¬¡æ€§åº”ç”¨æ‰€æœ‰ä¿®æ”¹ï¼Œæ¯æ­¥ååœ¨å°æ•°æ®é›†éªŒè¯ã€‚

### 2. è¶…å‚æ•°è°ƒä¼˜é¡ºåº

```
1. å…ˆå›ºå®š use_moco=falseï¼Œè°ƒä¼˜ batch_size, learning_rate
2. å¯ç”¨ use_moco=trueï¼Œå›ºå®šå…¶ä»–å‚æ•°
3. è°ƒä¼˜ moco_momentum (0.99, 0.995, 0.999)
4. è°ƒä¼˜ queue_size (4096, 8192, 16384)
```

### 3. ç›‘æ§æŒ‡æ ‡

è®­ç»ƒæ—¶é‡ç‚¹è§‚å¯Ÿï¼š
- **Loss æ›²çº¿**: åº”è¯¥å¹³æ»‘ä¸‹é™
- **æ¢¯åº¦èŒƒæ•°**: ä¸åº”è¯¥çˆ†ç‚¸æˆ–æ¶ˆå¤±
- **é˜Ÿåˆ—ä½¿ç”¨ç‡**: queue_ptr åº”è¯¥æ­£å¸¸å¾ªç¯
- **åŠ¨é‡ç¼–ç å™¨å·®å¼‚**: ä¸ä¸»ç¼–ç å™¨å‚æ•°åº”æœ‰å°å·®å¼‚

### 4. æ˜¾å­˜ä¼˜åŒ–æŠ€å·§

å¦‚æœä»ç„¶æ˜¾å­˜ä¸è¶³ï¼š
```yaml
# æ–¹æ¡ˆ 1: å‡å°é˜Ÿåˆ—
queue_size: 4096  # ä» 8192 å‡åŠ

# æ–¹æ¡ˆ 2: å‡å° batch
batch_size: 32
gradient_accumulation_steps: 8  # ä¿æŒæœ‰æ•ˆbatch=256

# æ–¹æ¡ˆ 3: å‡å°æ¨¡å‹
mamba_n_layers: 3  # ä» 4 å‡å°‘
cnn_channels: [32, 64, 128, 128]  # æœ€åä¸€å±‚ä¸å¢é•¿
```

---

## âœ… å®æ–½æ£€æŸ¥æ¸…å•

åœ¨æ­£å¼è®­ç»ƒå‰ï¼Œç¡®è®¤ï¼š

- [ ] `configs/pretrain.yaml` å·²æ›´æ–°æ‰€æœ‰å‚æ•°
- [ ] `use_moco` è®¾ç½®ç¬¦åˆé¢„æœŸ (true/false)
- [ ] `gradient_accumulation_steps` = 4
- [ ] `learning_rate` å·²æ ¹æ®æœ‰æ•ˆ batch size è°ƒæ•´
- [ ] `warmup_epochs` å·²å»¶é•¿
- [ ] å°æ•°æ®é›†æµ‹è¯•é€šè¿‡ï¼ˆ10 epochsï¼‰
- [ ] æ²¡æœ‰æ˜¾å­˜æº¢å‡ºé”™è¯¯
- [ ] Loss æ­£å¸¸ä¸‹é™
- [ ] é˜Ÿåˆ—æ­£å¸¸æ›´æ–°ï¼ˆå¦‚æœ use_moco=trueï¼‰

---

## ğŸ¯ æ€»ç»“

æœ¬æ¬¡ä¼˜åŒ–å®ç°äº†ï¼š

âœ… **5ä¸ªæ¸è¿›å¼æ­¥éª¤**ï¼Œæ¯æ­¥å¯ç‹¬ç«‹éªŒè¯  
âœ… **å‘åå…¼å®¹**ï¼Œå¯éšæ—¶åˆ‡æ¢ SimCLR/MoCo æ¨¡å¼  
âœ… **æ˜¾å­˜ä¼˜åŒ–**ï¼Œå•å¡ 24GB è¶³å¤Ÿ  
âœ… **è®­ç»ƒåŠ é€Ÿ**ï¼Œé¢„æœŸ 2-3 å€æå‡  
âœ… **æ ¸å¿ƒä¸å˜**ï¼Œåˆ†å±‚å¯¹æ¯”å­¦ä¹ æ¡†æ¶å®Œæ•´ä¿ç•™  

**æ¨èé…ç½®**: MoCo æ··åˆæ¨¡å¼ï¼ˆå‘¨æœŸçº§ MoCo + å­ç»“æ„çº§ SimCLRï¼‰  
**é€‚ç”¨åœºæ™¯**: RTX 4090D å•å¡ï¼Œæ•°æ®é›† >5000 æ ·æœ¬  
**é¢„æœŸæ”¶ç›Š**: è®­ç»ƒæ—¶é—´å‡åŠï¼Œæ€§èƒ½æå‡ 0.5-2%  

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2026-01-22  
**ä½œè€…**: PA-HCL Team  
